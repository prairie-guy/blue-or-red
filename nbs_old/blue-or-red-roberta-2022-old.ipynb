{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "855ccc90",
   "metadata": {},
   "source": [
    "## OLD:Blue-or-Red Roberta Transformer of 2022 Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9668712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:40:37.404165Z",
     "start_time": "2022-06-10T20:40:34.805838Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from ideology_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e737b0",
   "metadata": {},
   "source": [
    "### Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea58f3eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:40:38.522386Z",
     "start_time": "2022-06-10T20:40:38.519430Z"
    }
   },
   "outputs": [],
   "source": [
    "#model_nm = 'vinai/bertweet-base'\n",
    "model_nm = 'cardiffnlp/twitter-roberta-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f91961",
   "metadata": {},
   "source": [
    "### Build Dataset of 2022 Congressional Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8db458",
   "metadata": {},
   "source": [
    "#### Grab Tweets of Each Member of Congress for Each Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5693da6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:40:40.948165Z",
     "start_time": "2022-06-10T20:40:40.945066Z"
    }
   },
   "outputs": [],
   "source": [
    "path=Path('tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c751de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:40:49.945775Z",
     "start_time": "2022-06-10T20:40:46.000090Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat(map(tweets2df, path.rglob(\"tweets-congress*/*\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48409478",
   "metadata": {},
   "source": [
    "#### Label each tweet according by the handle of the legislator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c944214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:40:50.182166Z",
     "start_time": "2022-06-10T20:40:49.992202Z"
    }
   },
   "outputs": [],
   "source": [
    "df = label_tweets_of_legislators(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555f8f9",
   "metadata": {},
   "source": [
    "#### 2019 Tweets: ONLY PICK 2022 OR 2019 TWEETS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6dfafd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-04T22:20:18.971989Z",
     "start_time": "2022-06-04T22:20:11.011302Z"
    }
   },
   "outputs": [],
   "source": [
    "df = tweets2df(\"/home/cdaniels/fastai-projects/blue-or-red/data_full\") # 2019 tweets contain party affiliation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7afc200",
   "metadata": {},
   "source": [
    "#### Preproccess Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f1e7f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:40:54.649977Z",
     "start_time": "2022-06-10T20:40:53.151597Z"
    }
   },
   "outputs": [],
   "source": [
    "df = preprocess_tweets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fd87475",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:40:54.698796Z",
     "start_time": "2022-06-10T20:40:54.696053Z"
    }
   },
   "outputs": [],
   "source": [
    "def party2num(x):return 0 if x=='Democrat' else 1 # Democrate = 0 and Republican = 1; NEED TO BE INTEGERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00c5f371",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:40:55.489281Z",
     "start_time": "2022-06-10T20:40:55.476911Z"
    }
   },
   "outputs": [],
   "source": [
    "df.party = df.party.apply(party2num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7bd66",
   "metadata": {},
   "source": [
    "The label column must be called `labels`. By convention, also have text input called `input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "457672a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:40:57.918463Z",
     "start_time": "2022-06-10T20:40:57.912511Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={'text':'input', 'party':'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dadfe88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:44:03.689266Z",
     "start_time": "2022-06-10T20:44:03.681717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "      <th>input</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RepMikeLevin</td>\n",
       "      <td>The @user is doing incredible work in our community!I was proud to secure $150000 for the museum to fund childhood literacy programs. This funding will help advance elementary studentsâ€™ reading comprehension through active engagement with works of art. http</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RepMikeLevin</td>\n",
       "      <td>.@OutdoorAlliance is doing amazing work to protect our planet and expand access to public lands.During our meeting we discussed my American Coasts and Oceans Protection Act which would prohibit any new offshore drilling along the Southern California coast. http</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RepMikeLevin</td>\n",
       "      <td>I'm also glad that @user has announced he will delay any new tariffs on the American solar industry. I helped lead a letter last month urging the admin to make this commonsense decision so we can continue to support solar jobs lower energy costs and meet our climate goals.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RepMikeLevin</td>\n",
       "      <td>Investing in clean energy isn't just good for our planet it's good for our economy and our national security.Today @user invoked the Defense Production Act to ensure we have the energy we need to run our country all year round.https://t.co/g7ksT09v4l</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RepMikeLevin</td>\n",
       "      <td>Today we honor the heroic actions of the servicemembers who landed in Normandy 78 years ago and fought so hard to turn the tides of #WWII and protect our fundamental freedoms.Thank you for your service ðŸ‡ºðŸ‡¸https://t.co/J4Y64grM0T</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         handle  \\\n",
       "0  RepMikeLevin   \n",
       "1  RepMikeLevin   \n",
       "2  RepMikeLevin   \n",
       "3  RepMikeLevin   \n",
       "4  RepMikeLevin   \n",
       "\n",
       "                                                                                                                                                                                                                                                                               input  \\\n",
       "0                  The @user is doing incredible work in our community!I was proud to secure $150000 for the museum to fund childhood literacy programs. This funding will help advance elementary studentsâ€™ reading comprehension through active engagement with works of art. http   \n",
       "1              .@OutdoorAlliance is doing amazing work to protect our planet and expand access to public lands.During our meeting we discussed my American Coasts and Oceans Protection Act which would prohibit any new offshore drilling along the Southern California coast. http   \n",
       "2  I'm also glad that @user has announced he will delay any new tariffs on the American solar industry. I helped lead a letter last month urging the admin to make this commonsense decision so we can continue to support solar jobs lower energy costs and meet our climate goals.   \n",
       "3                         Investing in clean energy isn't just good for our planet it's good for our economy and our national security.Today @user invoked the Defense Production Act to ensure we have the energy we need to run our country all year round.https://t.co/g7ksT09v4l   \n",
       "4                                                Today we honor the heroic actions of the servicemembers who landed in Normandy 78 years ago and fought so hard to turn the tides of #WWII and protect our fundamental freedoms.Thank you for your service ðŸ‡ºðŸ‡¸https://t.co/J4Y64grM0T   \n",
       "\n",
       "   labels  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89004c8",
   "metadata": {},
   "source": [
    "### Create Dataset from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05dfa051",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:41:05.057147Z",
     "start_time": "2022-06-10T20:41:05.054097Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6317a1e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:41:06.827639Z",
     "start_time": "2022-06-10T20:41:06.803410Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['handle', 'input', 'labels'],\n",
       "    num_rows: 30107\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(df)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b927b4b",
   "metadata": {},
   "source": [
    "### Tokenize Dataset\n",
    "Need to know the `model` in order to get the correct Tokenizer with `AutoTokenizer(model)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47b7449c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:41:09.928323Z",
     "start_time": "2022-06-10T20:41:09.925141Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793c9492",
   "metadata": {},
   "source": [
    "**It is important that the tokenizer used for the training is the same as used for new documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b85e744",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:41:15.427437Z",
     "start_time": "2022-06-10T20:41:13.344801Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4443c5",
   "metadata": {},
   "source": [
    "Here's a simple function which tokenizes our inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3963324b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:41:17.780988Z",
     "start_time": "2022-06-10T20:41:17.777599Z"
    }
   },
   "outputs": [],
   "source": [
    "def tok_func(x): return tokz(x[\"input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286c74d",
   "metadata": {},
   "source": [
    "Map `tok_func` on `ds['input']`, which uses `tokz` to tokenize each element. This creates a tokenized dataset `tok_ds`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71184837",
   "metadata": {},
   "source": [
    "Error checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15d7b1ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:41:27.682982Z",
     "start_time": "2022-06-10T20:41:19.730288Z"
    }
   },
   "outputs": [],
   "source": [
    "for d in ds:\n",
    "    try:\n",
    "       tok_func(d)\n",
    "    except:\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a8bfd01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:41:30.089121Z",
     "start_time": "2022-06-10T20:41:28.686641Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adb2cf051bf4f3fa6ff2937c1018d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_ds = ds.map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed768b55",
   "metadata": {},
   "source": [
    "### Create the Training and Test/Valid datasets from tok_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e92cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T00:33:08.029512Z",
     "start_time": "2022-05-31T00:33:08.017497Z"
    }
   },
   "source": [
    "`DatasetDict` here, `dds` holds training and validation datasets. To create one that contains 25% of our data for the validation set, and 75% for the training set, use `train_test_split`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "839bcf82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:41:32.447913Z",
     "start_time": "2022-06-10T20:41:32.432710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['handle', 'input', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 22580\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['handle', 'input', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 7527\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dds = tok_ds.train_test_split(.25) # DataSetDict\n",
    "dds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00962bf1",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e7573",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T12:54:57.088146Z",
     "start_time": "2022-06-02T12:54:57.080801Z"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0140848b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T19:28:40.128123Z",
     "start_time": "2022-05-31T19:28:40.124567Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def compute_metrics(eval_pred):\n",
    "#    f1 = f1_score(*eval_pred, average=\"weighted\")\n",
    "#    acc = accuracy_score(*eval_pred)\n",
    "#    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4785a07c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T22:59:42.671551Z",
     "start_time": "2022-05-31T22:59:42.667450Z"
    }
   },
   "outputs": [],
   "source": [
    "#def compute_metrics(eval_pred):\n",
    "#    preds, labels = eval_pred\n",
    "#    preds = np.argmax(preds, axis=1)\n",
    "#    accuracy = sum(preds == labels)/len(preds)\n",
    "#    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23a7e8dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:41:36.024660Z",
     "start_time": "2022-06-10T20:41:36.020406Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f36bdd",
   "metadata": {},
   "source": [
    "Transformers expects metrics to be returned as a `dict`, since that way the trainer knows what label to use, so let's create a function to do that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a725bfb5",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a111cefb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:41:38.999758Z",
     "start_time": "2022-06-10T20:41:38.996321Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee89dc60",
   "metadata": {},
   "source": [
    "Another Auto Factory Method using the `model_nm` to create a model consisitent with the `AutoTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a253a0a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:41:43.816588Z",
     "start_time": "2022-06-10T20:41:41.956010Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=2) # KEY NUMBER FOR 2 Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841721b2",
   "metadata": {},
   "source": [
    "### Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70f01351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:41:46.674659Z",
     "start_time": "2022-06-10T20:41:46.671497Z"
    }
   },
   "outputs": [],
   "source": [
    "bs = 128\n",
    "epochs = 4\n",
    "lr = 8e-5\n",
    "#lr = 3e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbf5916",
   "metadata": {},
   "source": [
    "All of the paramaters related to the `Trainer` go into `TrainerArguments`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a083c4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:41:49.175143Z",
     "start_time": "2022-06-10T20:41:49.170453Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to='none', logging_strategy='epoch')\n",
    "# logging_strategy='epoch' required to get Training_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ee2e57f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:41:54.171087Z",
     "start_time": "2022-06-10T20:41:52.843847Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                  tokenizer=tokz, compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a7b8aa",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fb66acf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T20:43:38.111163Z",
     "start_time": "2022-06-10T20:41:57.056635Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: input, handle. If input, handle are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/cdaniels/mambaforge/envs/fastai/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 22580\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 356\n",
      "/home/cdaniels/mambaforge/envs/fastai/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='356' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/356 01:36 < 02:02, 1.61 it/s, Epoch 1.76/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.485900</td>\n",
       "      <td>0.355394</td>\n",
       "      <td>0.836057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: input, handle. If input, handle are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7527\n",
      "  Batch size = 512\n",
      "/home/cdaniels/mambaforge/envs/fastai/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/fastai/lib/python3.9/site-packages/transformers/trainer.py:1317\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1314\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1316\u001b[0m )\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/fastai/lib/python3.9/site-packages/transformers/trainer.py:1556\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1554\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m-> 1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1557\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1560\u001b[0m ):\n\u001b[1;32m   1561\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac0971c",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa257391",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-05T00:15:19.726800Z",
     "start_time": "2022-06-05T00:15:19.723336Z"
    }
   },
   "outputs": [],
   "source": [
    "roberta_model = 'blue-or-red-roberta-2022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "232897a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-05T00:15:25.624594Z",
     "start_time": "2022-06-05T00:15:25.422858Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in blue-or-red-roberta-2022/tokenizer_config.json\n",
      "Special tokens file saved in blue-or-red-roberta-2022/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('blue-or-red-roberta-2022/tokenizer_config.json',\n",
       " 'blue-or-red-roberta-2022/special_tokens_map.json',\n",
       " 'blue-or-red-roberta-2022/vocab.json',\n",
       " 'blue-or-red-roberta-2022/merges.txt',\n",
       " 'blue-or-red-roberta-2022/added_tokens.json',\n",
       " 'blue-or-red-roberta-2022/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokz.save_pretrained(roberta_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a02d94d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-05T00:15:34.334118Z",
     "start_time": "2022-06-05T00:15:31.461787Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in blue-or-red-roberta-2022/config.json\n",
      "Model weights saved in blue-or-red-roberta-2022/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(roberta_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753a608",
   "metadata": {},
   "source": [
    "## Create Test Dataset and Evaluate using 2022 Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d489b3",
   "metadata": {},
   "source": [
    "### Load Saved Model and Create Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4043f656",
   "metadata": {},
   "source": [
    "#### 1) Restarted Kernel: Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea9974d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-05T02:47:13.049044Z",
     "start_time": "2022-06-05T02:46:59.723303Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from ideology_utils import *\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e4b4e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-05T02:47:15.410627Z",
     "start_time": "2022-06-05T02:47:15.403626Z"
    }
   },
   "outputs": [],
   "source": [
    "def tok_func(x): return tokz(x[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765eeee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-05T02:47:17.150653Z",
     "start_time": "2022-06-05T02:47:17.143586Z"
    }
   },
   "outputs": [],
   "source": [
    "def party2num(x):return 0 if x=='Democrat' else 1 # Democrate = 0 and Republican = 1; NEED TO BE INTEGERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a608be7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-05T02:47:18.591784Z",
     "start_time": "2022-06-05T02:47:18.582510Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc8a359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-05T00:03:17.489725Z",
     "start_time": "2022-06-05T00:03:17.486845Z"
    }
   },
   "source": [
    "#### 2) Otherwise: Reload Saved Model from Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41f33c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-05T02:47:43.463722Z",
     "start_time": "2022-06-05T02:47:20.982205Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roberta_model = 'blue-or-red-roberta-2022'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta_model)\n",
    "tokz  = AutoTokenizer.from_pretrained(roberta_model)\n",
    "args = TrainingArguments(\"tmp_trainer\", per_device_eval_batch_size=128)\n",
    "trainer = Trainer(model, args, tokenizer=tokz);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df1507",
   "metadata": {},
   "source": [
    "### Create Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "32a024fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T04:04:21.378908Z",
     "start_time": "2022-06-09T04:04:21.371522Z"
    }
   },
   "outputs": [],
   "source": [
    "#weeks = ['tweets-congress-2022-05-18','tweets-congress-2022-05-24','tweets-congress-2022-06-01']\n",
    "weeks = ['tweets-congress-2022-06-01']\n",
    "weeks = [path/week for week in weeks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0d3f15ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T04:04:25.138327Z",
     "start_time": "2022-06-09T04:04:24.112544Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['handle','text']) \n",
    "for week in weeks:       \n",
    "    for t in Path(week).ls():\n",
    "        dft = pd.read_csv(t)\n",
    "        df  = pd.concat([df,dft],ignore_index=True)\n",
    "df = label_tweets_of_legislators(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ca448e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T04:04:38.704200Z",
     "start_time": "2022-06-09T04:04:38.395399Z"
    }
   },
   "outputs": [],
   "source": [
    "df = preprocess_tweets(df)\n",
    "df.party = df.party.apply(party2num)\n",
    "df = df.rename(columns={'text':'input', 'party':'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f0155991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T04:04:41.333608Z",
     "start_time": "2022-06-09T04:04:40.908451Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdd8eb5819048869d85a922ef902092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_df = df[['input']]\n",
    "eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fa706760",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T04:04:44.149521Z",
     "start_time": "2022-06-09T04:04:44.143023Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_labels = df.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5bcf3e",
   "metadata": {},
   "source": [
    "### Prediction and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e005c5",
   "metadata": {},
   "source": [
    "Use `trainer.predict(eval_ds)` to make predictions on the `eval_ds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9e3116ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T04:05:23.512926Z",
     "start_time": "2022-06-09T04:05:19.183309Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: input. If input are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5849\n",
      "  Batch size = 512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(eval_ds).predictions.astype(float)\n",
    "preds = torch.tensor(preds)\n",
    "preds = F.softmax(preds, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9120c3dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T04:05:27.023208Z",
     "start_time": "2022-06-09T04:05:26.698616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9446059155411182}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics([preds,eval_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eeb884",
   "metadata": {},
   "source": [
    "### Create our CSV Submission Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e87a642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T04:36:35.524516Z",
     "start_time": "2022-05-31T04:36:35.478277Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "submission = datasets.Dataset.from_dict({\n",
    "    'id': eval_ds['id'],\n",
    "    'score': preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8300999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T04:36:39.711579Z",
     "start_time": "2022-05-31T04:36:39.692026Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6388a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
